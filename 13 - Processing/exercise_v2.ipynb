{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_02",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_03",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = pd.read_csv(\"/Users/alexandreheringer/31-Other-Projects/Research/12 - Inputs/sample_data.csv\")\n",
    "\n",
    "print(source_data.head())\n",
    "print(source_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_04",
   "metadata": {},
   "source": [
    "## Technology Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_05",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_classification_conditions = [\n",
    "    (source_data['technology'] == 10) & (source_data['max_advertised_download_speed'] < 50),\n",
    "    (source_data['technology'] == 10) & (source_data['max_advertised_download_speed'] >= 50),\n",
    "    (source_data['technology'] == 40) & (source_data['max_advertised_download_speed'] < 500),\n",
    "    (source_data['technology'] == 40) & (source_data['max_advertised_download_speed'] >= 500),\n",
    "    (source_data['technology'] == 50)\n",
    "]\n",
    "\n",
    "technology_classification_labels = [\n",
    "    'Slow Copper',\n",
    "    'Fast Copper',\n",
    "    'Slow Cable',\n",
    "    'Fast Cable',\n",
    "    'Fiber'\n",
    "]\n",
    "\n",
    "source_data['technology_category'] = np.select(\n",
    "    technology_classification_conditions,\n",
    "    technology_classification_labels,\n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "print(source_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_06",
   "metadata": {},
   "source": "## Gigabit-Capable Provider Identification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_07",
   "metadata": {},
   "outputs": [],
   "source": "#Create function to check if it's gigabit:\n\n# If fiber then it's gigabit\n# If technology = 40 & download_speed >= 500\n# Vectorized implementation (replaces row-by-row apply for performance)\nsource_data['is_gigabit'] = (\n    (source_data['technology'] == 50)\n    | (\n        (source_data['technology'] == 40)\n        & (source_data['max_advertised_download_speed'] >= 500)\n    )\n).astype(int)\n\n# create check coulmn and apply function for each row\n# vectorized above instead of using apply()\n\n# Note: competition-related columns (current_gigabit_providers, available_spots,\n# competition_score) were previously computed here but are NOT needed by the\n# simulation — it recalculates everything from scratch each year.\n# They are now computed only in the Exploratory section below where they are used.\n\nprint(source_data.head())"
  },
  {
   "cell_type": "markdown",
   "id": "cell_08",
   "metadata": {},
   "source": [
    "## Identify Eligible Contenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_09",
   "metadata": {},
   "outputs": [],
   "source": "#1. UPGRADES: Providers already at the location with slow technology\n\n# Filter rows eligible for upgrade\n# Vectorized implementation\nupgrade_opportunities = source_data[\n    source_data['technology_category'].isin(['Slow Copper', 'Slow Cable'])\n].copy()\nupgrade_opportunities['growth_type'] = 'Upgrade'\n\n# Keep essential columns for the build queue\nupgrade_opportunities = upgrade_opportunities[\n    ['location_id', 'block_geoid', 'provider_id', 'growth_type']\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_10",
   "metadata": {},
   "outputs": [],
   "source": "#2. EDGE-OUT: Providers in the census block but not at this specific location\n\n# 1. Identify all unique providers present in each block\nproviders_per_block = source_data[['block_geoid', 'provider_id']].drop_duplicates()\n\n# 2. Identify all unique locations in each block\nlocations_per_block = source_data[\n    ['location_id', 'block_geoid']\n].drop_duplicates()\n\n# 3. Cartesian Product (within blocks): Every provider in the block mapped to every location in that block\npotential_edge_out_pairs = locations_per_block.merge(\n    providers_per_block, on='block_geoid'\n)\n\n# 4. Filter out combinations where the provider is already physically present at the location\nexisting_presence = source_data[['location_id', 'provider_id']].assign(is_present=True)\nedge_out_opportunities = potential_edge_out_pairs.merge(\n    existing_presence, on=['location_id', 'provider_id'], how='left'\n)\n\n# Keep only the 'NaN' results (where is_present is null)\nedge_out_opportunities = edge_out_opportunities[\n    edge_out_opportunities['is_present'].isna()\n].copy()\nedge_out_opportunities['growth_type'] = 'Edge-Out'\nedge_out_opportunities = edge_out_opportunities.drop(columns=['is_present'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Unify All Contenders ---\n",
    "# Combine Upgrade and Edge-Out candidates into a single master table for scoring\n",
    "all_contenders = pd.concat(\n",
    "    [upgrade_opportunities, edge_out_opportunities], ignore_index=True\n",
    ")\n",
    "\n",
    "print(f'Total Upgrade opportunities found: {len(upgrade_opportunities)}')\n",
    "print(f'Total Edge-Out opportunities found: {len(edge_out_opportunities)}')\n",
    "print(all_contenders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_12",
   "metadata": {},
   "source": [
    "## Exploratory: Pre-Simulation Scoring Verification\n",
    "\n",
    "> **Note:** This section computes the composite score on the initial contenders dataset\n",
    "> for verification and exploration purposes only. The actual multi-year simulation below\n",
    "> recalculates all scores from scratch each year based on the updated market state.\n",
    "> This section can be safely skipped without affecting the simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Calculate Block Market Size\n",
    "# Count how many unique locations are in each block\n",
    "block_location_counts = (\n",
    "    source_data.groupby('block_geoid')['location_id'].nunique().reset_index()\n",
    ")\n",
    "block_location_counts.columns = ['block_geoid', 'location_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Calculation Provider Gigabit Coverage per Block\n",
    "# Count how many gigabit locations a provider already has in each block\n",
    "gigabit_counts_per_block = (\n",
    "    source_data.groupby(['provider_id', 'block_geoid'])['is_gigabit']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "gigabit_counts_per_block.columns = [\n",
    "    'provider_id', 'block_geoid', 'gigabit_location_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_15",
   "metadata": {},
   "outputs": [],
   "source": "#3: Combine scores into the contenders table\nall_contenders = all_contenders.merge(\n    block_location_counts, on='block_geoid', how='left'\n)\nall_contenders = all_contenders.merge(\n    gigabit_counts_per_block, on=['provider_id', 'block_geoid'], how='left'\n).fillna(0)\n\n# Competition score (computed here for exploratory purposes only;\n# the simulation recalculates this from scratch each year)\n# counts and apply results to respective correct line\nexploratory_gigabit_counts = (\n    source_data.groupby('location_id')['is_gigabit'].sum().reset_index()\n)\nexploratory_gigabit_counts.columns = ['location_id', 'current_gigabit_providers']\nall_contenders = all_contenders.merge(\n    exploratory_gigabit_counts, on='location_id', how='left'\n)\nall_contenders['current_gigabit_providers'] = (\n    all_contenders['current_gigabit_providers'].fillna(0)\n)\n# Calculate and the spot availability\nall_contenders['available_spots'] = 2 - all_contenders['current_gigabit_providers']\n# if value is negative then == 0(grandfathered)\nall_contenders.loc[all_contenders['available_spots'] < 0, 'available_spots'] = 0\n#Create competition score normalizing available spots\nall_contenders['competition_score'] = all_contenders['available_spots'] / 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Normalize scores to 0-1 scale\n",
    "# Normalize Market Size\n",
    "min_size = all_contenders['location_count'].min()\n",
    "max_size = all_contenders['location_count'].max()\n",
    "all_contenders['market_size_score'] = (\n",
    "    (all_contenders['location_count'] - min_size) / (max_size - min_size)\n",
    ")\n",
    "\n",
    "# Calculate and Normalize Coverage Ratio\n",
    "all_contenders['coverage_ratio'] = (\n",
    "    all_contenders['gigabit_location_count'] / all_contenders['location_count']\n",
    ")\n",
    "min_coverage = all_contenders['coverage_ratio'].min()\n",
    "max_coverage = all_contenders['coverage_ratio'].max()\n",
    "all_contenders['gigabit_coverage_score'] = (\n",
    "    (all_contenders['coverage_ratio'] - min_coverage)\n",
    "    / (max_coverage - min_coverage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Composite Score: Average of the three components\n",
    "all_contenders['composite_score'] = (\n",
    "    all_contenders['competition_score']\n",
    "    + all_contenders['gigabit_coverage_score']\n",
    "    + all_contenders['market_size_score']\n",
    ") / 3\n",
    "\n",
    "# Sort by priority (Highest score first)\n",
    "all_contenders = all_contenders.sort_values(\n",
    "    by='composite_score', ascending=False\n",
    ")\n",
    "\n",
    "print('Top prioritized build opportunities (exploratory):')\n",
    "print(\n",
    "    all_contenders[\n",
    "        ['location_id', 'provider_id', 'growth_type', 'composite_score']\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_18",
   "metadata": {},
   "source": [
    "## MULTI-YEAR SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Simulation setup\n",
    "#Creates a database to store the market state over the years\n",
    "market_state = source_data.copy()\n",
    "\n",
    "#creates the log of all construction events\n",
    "build_history = []\n",
    "\n",
    "#calculates the state annual building capacity \n",
    "total_unique_locations = market_state['location_id'].nunique()\n",
    "state_annual_capacity = int(0.02 * total_unique_locations)\n",
    "\n",
    "#2: Calculation of all possible Edge-Outs\n",
    "# If a provider is on the block, he will always be a possible Edge-Out\n",
    "# What changes over years is if he already built (then exit the list) or not.\n",
    "\n",
    "providers_per_block_static = (\n",
    "    market_state[['block_geoid', 'provider_id']].drop_duplicates()\n",
    ")\n",
    "locations_per_block_static = (\n",
    "    market_state[['location_id', 'block_geoid']].drop_duplicates()\n",
    ")\n",
    "\n",
    "#Total universe of edge outs\n",
    "edge_out_universe = locations_per_block_static.merge(\n",
    "    providers_per_block_static, on='block_geoid'\n",
    ")\n",
    "edge_out_universe['growth_type'] = 'Edge-Out'\n",
    "\n",
    "# Block location counts dictionary (static throughout the simulation)\n",
    "block_location_counts_dict = (\n",
    "    market_state.groupby('block_geoid')['location_id'].nunique().to_dict()\n",
    ")\n",
    "\n",
    "print(f'Total unique locations: {total_unique_locations}')\n",
    "print(f'State annual capacity: {state_annual_capacity} builds/year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_20",
   "metadata": {},
   "outputs": [],
   "source": "#3: Simulation Loop\nfor year in range(2026, 2036):\n    print(f\"\\n{'=' * 60}\")\n    print(f\">>> Simulating Year: {year}\")\n    print(f\"{'=' * 60}\")\n\n    #3.1 - Establish Capacities\n\n    #Counts how many unique locations each provider serves acctualy.\n    provider_footprints = market_state.groupby('provider_id')['location_id'].nunique()\n\n    #Variable provider_caps -> increases over time,\n    #current_state_remaining -> gets reseted to the same fixed level.\n    provider_capacities = (provider_footprints * 0.04).astype(int).to_dict()\n    state_remaining_capacity = state_annual_capacity\n\n    #3.2: Fast Trackers:\n    #Creates a dictionary with location id and how many Gigabits providers exists on it.\n    #used to check the rule of limit of 2 providers per location.\n    location_gigabit_counts = (\n        market_state.groupby('location_id')['is_gigabit'].sum().to_dict()\n    )\n\n    #Used to calculate Coverage Score.\n    # Understands if the provider already is strong in the block, in order to prioritize it.\n    provider_gigabit_by_block = (\n        market_state.groupby(['provider_id', 'block_geoid'])['is_gigabit']\n        .sum()\n        .reset_index()\n        .rename(columns={'is_gigabit': 'provider_gigabit_count_in_block'})\n    )\n\n    # Used to rank contenders within each location (tie-breaking)\n    provider_total_locations_by_block = (\n        market_state.groupby(['provider_id', 'block_geoid'])['location_id']\n        .nunique()\n        .reset_index()\n        .rename(columns={'location_id': 'provider_total_locations_in_block'})\n    )\n\n    #3.3: Identify Contenders\n    #A. UPGRADES -  Identify who should upgrade each year. Changes YoY.\n    upgrade_candidates = market_state[\n        market_state['technology_category'].isin(['Slow Copper', 'Slow Cable'])\n    ][['location_id', 'block_geoid', 'provider_id']].copy()\n    upgrade_candidates['growth_type'] = 'Upgrade'\n\n    #B. EDGE-OUTS\n    # Creates a set of al the locations,providers on the actual market.\n\n    #Instead of using this function below, I've decided to use a vector approach in order to save memory\n    #def filter_existing(row):\n    #    return (row['location_id'], row['provider_id']) not in existing_pairs\n\n    #Select universe, select existing locations and providers and creates a mask with the subtraction\n    #generating mask_not_exists variable.\n    universe_index = pd.MultiIndex.from_frame(\n        edge_out_universe[['location_id', 'provider_id']]\n    )\n    existing_index = pd.MultiIndex.from_frame(\n        market_state[['location_id', 'provider_id']]\n    )\n    not_yet_present_mask = ~universe_index.isin(existing_index)\n\n    #creates a valid list of edge out expansions for this year.\n    edge_out_candidates = edge_out_universe[not_yet_present_mask].copy()\n\n    #unify upgrades and edge out cases for this year.\n    candidates = pd.concat(\n        [upgrade_candidates, edge_out_candidates], ignore_index=True\n    )\n\n    if candidates.empty:\n        print(\"No more build opportunities. Stopping simulation.\")\n        break\n\n    #3.4:Update competition score to check who has priority.\n    candidates['current_gigabit_count'] = (\n        candidates['location_id'].map(location_gigabit_counts).fillna(0)\n    )\n    candidates['competition_score'] = (\n        (2 - candidates['current_gigabit_count']).clip(lower=0) / 2\n    )\n\n    #insert market size into candidates df and normalize market score.\n    candidates['block_location_count'] = candidates['block_geoid'].map(\n        block_location_counts_dict\n    )\n    min_size = candidates['block_location_count'].min()\n    max_size = candidates['block_location_count'].max()\n    candidates['market_size_score'] = (\n        (candidates['block_location_count'] - min_size)\n        / (max_size - min_size + 1e-9)\n    )\n\n    # Calculates the Coverage Score (location occupation by provider) and normalize it.\n    # --- R2 Performance: replaced .apply(axis=1) with merge ---\n    # Why: .apply() runs a Python-level loop over every row (~138k candidates/year).\n    # merge() delegates the join to pandas' optimized C internals, which is\n    # significantly faster. The result is identical: each candidate row gets the\n    # provider's gigabit count in its block, looked up by (provider_id, block_geoid).\n    candidates = candidates.merge(\n        provider_gigabit_by_block,\n        on=['provider_id', 'block_geoid'],\n        how='left',\n    )\n    candidates['provider_gigabit_count_in_block'] = (\n        candidates['provider_gigabit_count_in_block'].fillna(0)\n    )\n\n    candidates['coverage_ratio'] = (\n        candidates['provider_gigabit_count_in_block']\n        / candidates['block_location_count']\n    )\n    min_coverage = candidates['coverage_ratio'].min()\n    max_coverage = candidates['coverage_ratio'].max()\n    candidates['gigabit_coverage_score'] = (\n        (candidates['coverage_ratio'] - min_coverage)\n        / (max_coverage - min_coverage + 1e-9)\n    )\n\n    #Final Ranking (average between the 3 scores).\n    candidates['composite_score'] = (\n        candidates['competition_score']\n        + candidates['gigabit_coverage_score']\n        + candidates['market_size_score']\n    ) / 3\n\n    # Tie-breaking columns for within-location contender prioritization.\n    # --- R2 Performance: replaced .apply(axis=1) with merge (same rationale as above) ---\n    candidates = candidates.merge(\n        provider_total_locations_by_block,\n        on=['provider_id', 'block_geoid'],\n        how='left',\n    )\n    candidates['provider_total_locations_in_block'] = (\n        candidates['provider_total_locations_in_block'].fillna(0)\n    )\n\n    candidates['type_priority'] = candidates['growth_type'].map(\n        {'Upgrade': 0, 'Edge-Out': 1}\n    )\n\n    #3.5 Allocation Process (Greedy Algorithm)\n    # --- R3 Performance: Pre-group candidates by location ---\n    # Why: Previously, the loop filtered `candidates[candidates['location_id'] == X]`\n    # for every location, scanning the full DataFrame each time (O(locations × candidates)).\n    # By grouping once upfront and pre-sorting each group, the allocation loop\n    # simply iterates through pre-built lists — no repeated filtering or sorting.\n    candidates_by_location = {}\n    for loc_id, group in candidates.groupby('location_id'):\n        candidates_by_location[loc_id] = group.sort_values(\n            by=[\n                'type_priority',\n                'provider_gigabit_count_in_block',\n                'provider_total_locations_in_block',\n            ],\n            ascending=[True, False, False],\n        )\n\n    # Determine location processing order by maximum composite score at each location\n    location_processing_order = (\n        candidates.groupby('location_id')['composite_score']\n        .max()\n        .sort_values(ascending=False)\n        .index\n    )\n\n    year_build_log = []\n    new_edge_out_rows = []  # List to accumulate new records for bulk insertion\n\n    # Iterate through candidates ranked by priority (Highest Score -> Lowest Score)\n    # Now iterating location-by-location, trying all contenders per location before moving on\n    for current_location_id in location_processing_order:\n        # Check State Constraint: Stop entire year if state cap is hit\n        if state_remaining_capacity <= 0:\n            break\n\n        # Check Saturation Constraint: Skip if location already has 2+ Gigabit providers\n        if location_gigabit_counts.get(current_location_id, 0) >= 2:\n            continue\n\n        # Get contenders for this location (pre-sorted by priority):\n        # 1. Upgrades before edge-outs (type_priority ascending)\n        # 2. Higher gigabit count in block (descending)\n        # 3. Higher total locations in block (descending)\n        location_contenders = candidates_by_location[current_location_id]\n\n        for _, contender in location_contenders.iterrows():\n            current_provider_id = contender['provider_id']\n\n            # Check Provider Constraint: Skip if provider hit their 4% annual limit.\n            if provider_capacities.get(current_provider_id, 0) <= 0:\n                continue\n\n            # Build Confirmed: Record the transaction\n            year_build_log.append({\n                'year': year,\n                'provider_id': current_provider_id,\n                'location_id': current_location_id,\n                'growth_type': contender['growth_type'],\n            })\n\n            # Update Real-Time Constraints (decrement caps, increment saturation).\n            state_remaining_capacity -= 1\n            provider_capacities[current_provider_id] -= 1\n            location_gigabit_counts[current_location_id] = (\n                location_gigabit_counts.get(current_location_id, 0) + 1\n            )\n\n            # Update Market State (for next year's logic).\n            if contender['growth_type'] == 'Upgrade':\n                # For Upgrades: Locate existing row and update technology to Fiber.\n                upgrade_mask = (\n                    (market_state['location_id'] == current_location_id)\n                    & (market_state['provider_id'] == current_provider_id)\n                )\n                market_state.loc[\n                    upgrade_mask,\n                    ['technology', 'technology_category', 'is_gigabit'],\n                ] = [50, 'Fiber', 1]\n            else:\n                # For Edge-Outs: Stage new service row for bulk insertion.\n                new_edge_out_rows.append({\n                    'location_id': current_location_id,\n                    'block_geoid': contender['block_geoid'],\n                    'provider_id': current_provider_id,\n                    'technology': 50,\n                    'max_advertised_download_speed': 1000,\n                    'max_advertised_upload_speed': 1000,\n                    'technology_category': 'Fiber',\n                    'is_gigabit': 1,\n                })\n\n            # One build per available spot; move to next location.\n            break\n\n    #Bulk insert all new Edge-Out rows at once.\n    if new_edge_out_rows:\n        market_state = pd.concat(\n            [market_state, pd.DataFrame(new_edge_out_rows)], ignore_index=True\n        )\n\n    build_history.extend(year_build_log)\n\n    upgrade_count = sum(\n        1 for build in year_build_log if build['growth_type'] == 'Upgrade'\n    )\n    edge_out_count = sum(\n        1 for build in year_build_log if build['growth_type'] == 'Edge-Out'\n    )\n    print(\n        f\"Year {year} finished: {len(year_build_log)} works carried out. \"\n        f\"({upgrade_count} upgrades, {edge_out_count} edge-outs)\"\n    )\n\n    if len(year_build_log) == 0:\n        print(\"No builds possible this year. Stopping simulation.\")\n        break"
  },
  {
   "cell_type": "markdown",
   "id": "cell_21",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_history:\n",
    "    # Convert the list of build dictionaries into a DataFrame.\n",
    "    build_history_dataframe = pd.DataFrame(build_history)\n",
    "\n",
    "    # Aggregate builds by year, provider, and type.\n",
    "    # 'unstack' pivots the growth_type to columns (Upgrade, Edge-Out).\n",
    "    summary = (\n",
    "        build_history_dataframe\n",
    "        .groupby(['year', 'provider_id', 'growth_type'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Rename columns to match the required output format.\n",
    "    summary = summary.rename(columns={\n",
    "        'Upgrade': 'upgrade_builds',\n",
    "        'Edge-Out': 'edge_out_builds'\n",
    "    })\n",
    "\n",
    "    # Ensure both column types exist (even if 0 builds of that type occurred).\n",
    "    for column_name in ['upgrade_builds', 'edge_out_builds']:\n",
    "        if column_name not in summary.columns:\n",
    "            summary[column_name] = 0\n",
    "\n",
    "    # Calculate total builds per provider per year.\n",
    "    summary['total_builds'] = (\n",
    "        summary['upgrade_builds'] + summary['edge_out_builds']\n",
    "    )\n",
    "\n",
    "    # Sort by year (ascending) and total builds (descending).\n",
    "    print('\\n--- Year-Over-Year Summary Table ---')\n",
    "    print(\n",
    "        summary.sort_values(\n",
    "            ['year', 'total_builds'], ascending=[True, False]\n",
    "        ).to_string()\n",
    "    )\n",
    "else:\n",
    "    print('No builds were recorded during the simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary table to CSV.\n",
    "output_directory = (\n",
    "    \"/Users/alexandreheringer/31-Other-Projects/Research/14 - Outputs\"\n",
    ")\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_directory, 'yoy_summary.csv')\n",
    "summary.to_csv(output_path, index=False)\n",
    "print(f'Summary table exported to: {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_24",
   "metadata": {},
   "source": "## Analysis Report\n\n### Which providers are growing the fastest and why?\n\nPROV_005 leads with 1,039 total builds over 10 years, followed by PROV_001 (992), PROV_007 (979), PROV_008 (962), and PROV_011 (937). These providers grow fastest because the 4% annual capacity rule scales directly with footprint size — PROV_005 starts with the largest footprint (capacity of 358 builds in Year 1) and each year's builds expand its footprint further, creating a compounding advantage. Conversely, smaller-footprint providers like PROV_010 (509 total) and PROV_012 (474 total) consistently receive fewer build allocations because their 4% cap is smaller.\n\n### Which growth type (upgrades vs. edge-out) dominates in early years vs. later years?\n\nUpgrades dominate exclusively in Year 1 (all 1,219 builds are upgrades, 0 edge-outs), since the specification gives upgrades strict priority over edge-outs and there are ~12,163 initial upgrade opportunities. Edge-outs first overtake upgrades in Year 2 (741 edge-outs vs. 478 upgrades). By Year 5 (2030), the split is 1,029 edge-outs vs. 190 upgrades. Upgrades never fully exhaust during the 10-year simulation (234 remain in Year 10), because some upgrade opportunities are at lower-scored locations that are continually outranked by higher-scored edge-out candidates.\n\n### What happens to the state-level and provider-level capacity constraints over time?\n\nThe state-level capacity (1,219 builds/year = 2% of 60,961 locations) is the binding constraint in every single year of the simulation — all 10 years hit exactly 1,219 builds. Provider-level capacities, starting at ~292–358 builds/year, grow via compounding as each year's builds expand footprints. Their aggregate sum far exceeds the state ceiling, meaning individual provider caps are never the bottleneck at the system level, though they do shape which providers receive allocations in any given year."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}